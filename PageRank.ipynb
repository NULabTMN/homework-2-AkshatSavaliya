{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PageRank.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/homework-2-AkshatSavaliya/blob/main/PageRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2QKFIlR6HCY"
      },
      "source": [
        "# PageRank\n",
        "\n",
        "In this assignment, you will compute PageRank on a collection of 469,235 web sites using the iterative version of the PageRank algorithm described in class for sparse graphs (NOT the power method with explicit matrix multiplication).\n",
        "\n",
        "Consider the following directed graph:\n",
        "\n",
        "![A directed link graph](https://ccs.neu.edu/home/dasmith/courses/cs6200/pagerank.jpg)\n",
        "\n",
        "We can represent this graph as a collection of nodes, here, ordered pairs of node index and node name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D_Mxj5pXKPl"
      },
      "source": [
        "small_nodes = [(0, 'A'),\n",
        "              (1, 'B'),\n",
        "              (2, 'C'),\n",
        "              (3, 'D'),\n",
        "              (4, 'E'),\n",
        "              (5, 'F'),\n",
        "               (6, 'G'),\n",
        "               (7,'H')]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTF3JKtTYxiZ"
      },
      "source": [
        "and a collection of directed links, i.e., ordered pairs from source to target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0V5ueOYDDN"
      },
      "source": [
        "small_edges = [\n",
        "  (0, 1),\n",
        "  (0, 2),\n",
        "  (0, 5),\n",
        "  (1, 2),\n",
        "  (1, 3),\n",
        "  (1, 4),\n",
        "  (1, 5),\n",
        "  (2, 3),\n",
        "  (2, 4),\n",
        "  (3, 0),\n",
        "  (3, 2),\n",
        "  (3, 4),\n",
        "  (3, 5),\n",
        "  (4, 0),\n",
        "  (5, 0),\n",
        "  (5, 1),\n",
        "  (5, 4),\n",
        "  (5, 6),\n",
        "  (4, 6)\n",
        "]\n",
        "\n",
        "\n",
        "xx = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBVDeszXY4B_"
      },
      "source": [
        "We use integer identifiers for the nodes for efficiency. Note that, unlike this example, in a real web graph, not every page will have in-links, nor will every page have out-links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPNsTGSsEwMX"
      },
      "source": [
        "## First Implementation and Test\n",
        "\n",
        "\\[10 points\\] Implement the iterative PageRank algorithm. Test your code on the six-node example using the input representation given above.  Be sure that your code handles pages that have no in-links or out-links properly.  (You may wish to test on a few such examples.) In later parts of this assignment, depending on how you store the data, it may be convenient to use iterators rather than storing the data in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMu_WaDA55sk",
        "outputId": "94ce6221-3a53-4477-a9b9-625077aa42fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Implement PageRank, given nodes and edges, to start with a uniform\n",
        "# distribution over nodes, run a fixed number of iterations, and\n",
        "# return a distribution over nodes.\n",
        "\n",
        "\n",
        "outlinks = {}\n",
        "\n",
        "for node in small_nodes:\n",
        "  if node[0] not in outlinks:\n",
        "    outlinks[node[0]] = []\n",
        "\n",
        "for item in small_edges:\n",
        "  outlinks[item[0]].append(item[1])\n",
        "\n",
        "print(outlinks)\n",
        "\n",
        "\n",
        "def page_rank_fixed_iter(nodes, edges, iterations=10):\n",
        "  P = len(nodes)\n",
        "  I = [0] * P\n",
        "  R = [0] * P\n",
        "\n",
        "  for i in range(P):\n",
        "    I[i] = 1 / P\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    for i in range(P):\n",
        "      R[i] = 0.15 / P\n",
        "    \n",
        "    final_val = 0\n",
        "\n",
        "    for node in nodes:\n",
        "      Q = outlinks[node[0]]\n",
        "      count = 0\n",
        "      if len(Q):\n",
        "        for q in Q:\n",
        "          R[q] += (1 - 0.15) * I[node[0]] / len(Q)\n",
        "      else:\n",
        "        count += 1\n",
        "        # for q in nodes:\n",
        "        #   R[q[0]] += (1-0.15) * I[node[0]] / P \n",
        "    \n",
        "      \n",
        "      final_val += count * ((1-0.15) * I[node[0]] / P)\n",
        "      # for q in nodes:\n",
        "      #   R[q[0]] += count * ((1-0.15) * I[node[0]] / P)\n",
        "    \n",
        "    for q in nodes:\n",
        "      R[q[0]] += final_val\n",
        "\n",
        "    for i in range(P):\n",
        "      I[i] = R[i]\n",
        "  \n",
        "  print(sum(R))\n",
        "  return R\n",
        "      \n",
        "\n",
        "# Output PageRank on the toy graph at various points.\n",
        "# Make sure your output has node number, name, and PageRank value.\n",
        "print(page_rank_fixed_iter(small_nodes, small_edges, 1))\n",
        "print(page_rank_fixed_iter(small_nodes, small_edges, 10))\n",
        "print(page_rank_fixed_iter(small_nodes, small_edges, 100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [1, 2, 5], 1: [2, 3, 4, 5], 2: [3, 4], 3: [0, 2, 4, 5], 4: [0, 6], 5: [0, 1, 4, 6], 6: [], 7: []}\n",
            "0.9999999999999999\n",
            "[0.1515625, 0.10729166666666667, 0.13385416666666666, 0.125, 0.17812499999999998, 0.13385416666666666, 0.125, 0.0453125]\n",
            "0.9999999999999999\n",
            "[0.1625252777388798, 0.11143691705643147, 0.1319584667231671, 0.11710561740876502, 0.1700293876530271, 0.1319584667231671, 0.1376426670894, 0.037343199607162245]\n",
            "0.9999999999999994\n",
            "[0.16253046791326042, 0.11143369232324157, 0.1319570611832602, 0.11710392820128275, 0.17002938844549811, 0.1319570611832602, 0.13764588317048784, 0.03734251757970834]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4duRjzABB9n"
      },
      "source": [
        "## PageRank on Web Crawl Data\n",
        "\n",
        "\\[20 points\\] Download and unpack a list of `.edu` websites and the links among them from the [Common Crawl](https://commoncrawl.org/2017/05/hostgraph-2017-feb-mar-apr-crawls/) open-source web crawl. For the sake of brevity, the data record links among websites, not web pages. The information for nodes and links is the same as the toy example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6EDDdTQCd3y",
        "outputId": "bc18869f-deeb-4cd6-d60a-4e4dc2a9ce4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If you're running on a machine (e.g., Windows) that doesn't have wget or gzip,\n",
        "# feel free to comment this out and use a different set of commands to load\n",
        "# the data.\n",
        "!wget https://ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz\n",
        "!gzip -df vertices-edu.txt.gz\n",
        "!wget https://ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz\n",
        "!gzip -df edges-edu.txt.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-09 16:24:50--  https://ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz\n",
            "Resolving ccs.neu.edu (ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to ccs.neu.edu (ccs.neu.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3703486 (3.5M) [application/x-gzip]\n",
            "Saving to: ‘vertices-edu.txt.gz’\n",
            "\n",
            "vertices-edu.txt.gz 100%[===================>]   3.53M  2.75MB/s    in 1.3s    \n",
            "\n",
            "2022-03-09 16:24:52 (2.75 MB/s) - ‘vertices-edu.txt.gz’ saved [3703486/3703486]\n",
            "\n",
            "--2022-03-09 16:24:53--  https://ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz\n",
            "Resolving ccs.neu.edu (ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to ccs.neu.edu (ccs.neu.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12829526 (12M) [application/x-gzip]\n",
            "Saving to: ‘edges-edu.txt.gz’\n",
            "\n",
            "edges-edu.txt.gz    100%[===================>]  12.23M  5.81MB/s    in 2.1s    \n",
            "\n",
            "2022-03-09 16:24:56 (5.81 MB/s) - ‘edges-edu.txt.gz’ saved [12829526/12829526]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW4yp1gPUwzb"
      },
      "source": [
        "There should now be files `vertices-edu.txt` and `edges-edu.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly1t9fyjK7eC"
      },
      "source": [
        "# TODO: Process the raw data into the same format as the simple graph.\n",
        "# You may create lists or iterators.\n",
        "import csv\n",
        "with open(\"vertices-edu.txt\") as load_file:\n",
        "    reader = csv.reader(load_file, delimiter=\" \")\n",
        "    m = [tuple(r) for r in reader]\n",
        "new_node = tuple((int(x[0]), x[1]) for x in m)\n",
        "\n",
        "import csv\n",
        "with open(\"edges-edu.txt\") as load_file:\n",
        "    reader = csv.reader(load_file, delimiter=\" \")\n",
        "    new_edges = [tuple(row) for row in reader]\n",
        "edges_p = tuple((int(x[0]), int(x[1])) for x in new_edges)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_node[:5])\n",
        "print(edges_p[:5])"
      ],
      "metadata": {
        "id": "mEY3bRSpwXlN",
        "outputId": "066cff61-6c6c-4841-f2d0-069c88fe0e19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((0, 'edu.00zl5e'), (1, 'edu.06hxbt'), (2, 'edu.082ifc'), (3, 'edu.083mjs'), (4, 'edu.09xzrr'))\n",
            "((386, 440), (19202, 1033), (103884, 2635), (342306, 7399), (8366, 8312))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WMf5L5VEqZb"
      },
      "source": [
        "Refine your implementation of PageRank to test for numerical convergence. Specificially, at each iteration, calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. (Recall that we defined entropy when talking about data compression.) The maximum perplexity of a PageRank distribution will therefore be the number of nodes in the graph.\n",
        "\n",
        "At each iteration, check the _change_ in perplexity. If the change is less than some threshold, you can stop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0yQKvKqAC",
        "outputId": "0159a56b-b801-4685-bea6-04bf0ffbe1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Implement convergence testing in PageRank\n",
        "# If you choose, you can share some subroutines with your first version.\n",
        "# Print the change in perplexity at each iteration.\n",
        "\n",
        "outlinks = {}\n",
        "\n",
        "for node in new_node:\n",
        "  if node[0] not in outlinks:\n",
        "    outlinks[node[0]] = []\n",
        "\n",
        "for item in edges_p:\n",
        "  outlinks[item[0]].append(item[1])\n",
        "\n",
        "import math\n",
        "\n",
        "def get_perplexity(I):\n",
        "    hpr = 0\n",
        "    for page in I:\n",
        "        hpr += page*(math.log2(page))\n",
        "    return 2**hpr\n",
        "\n",
        "\n",
        "def page_rank(nodes, edges, threshold=1):\n",
        "  P = len(nodes)\n",
        "  I = [0] * P\n",
        "  R = [0] * P\n",
        "\n",
        "  for i in range(P):\n",
        "    I[i] = 1 / P\n",
        "  \n",
        "  for i in range(15):\n",
        "    for i in range(P):\n",
        "      R[i] = 0.15 / P\n",
        "    \n",
        "    final_val = 0\n",
        "\n",
        "    for node in nodes:\n",
        "      Q = outlinks[node[0]]\n",
        "      count = 0\n",
        "      if len(Q):\n",
        "        for q in Q:\n",
        "          R[q] += (1 - 0.15) * I[node[0]] / len(Q)\n",
        "      else:\n",
        "        count += 1 \n",
        "     \n",
        "      final_val += count * ((1-0.15) * I[node[0]] / P)\n",
        "\n",
        "    \n",
        "    for q in nodes:\n",
        "      R[q[0]] += final_val\n",
        "\n",
        "    for i in range(P):\n",
        "      I[i] = R[i]\n",
        "    \n",
        "    print(get_perplexity(I))\n",
        "  \n",
        "  print(sum(R))\n",
        "\n",
        "  return R\n",
        "      \n",
        "\n",
        "# Run until perplexity changes by less than 1\n",
        "PR = page_rank(new_node, edges_p, 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.354900353014811e-06\n",
            "3.910178281925948e-06\n",
            "4.268902029324276e-06\n",
            "4.425249562550732e-06\n",
            "4.5138779226088696e-06\n",
            "4.5551038166181605e-06\n",
            "4.580832680156119e-06\n",
            "4.594069425897551e-06\n",
            "4.603245899508705e-06\n",
            "4.608308020240469e-06\n",
            "4.612222051557467e-06\n",
            "4.614493030990435e-06\n",
            "4.616398779324345e-06\n",
            "4.6175615052796726e-06\n",
            "4.618591448145051e-06\n",
            "0.9999999999981641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcncY2QHNl0M"
      },
      "source": [
        "## Link Analysis\n",
        "\n",
        "\\[20 points\\] In this final section, you will compute some properties of the web-site graph you downloaded.\n",
        "\n",
        "First, consider the _in-link count_ of a website, simply the number of web-sites pointing to it (including self-links). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FyPlLSO2bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2926f9-4342-41b0-f297-9ba7f11ddeaa"
      },
      "source": [
        "# TODO: List the document ID, domain name, and in-link count of the 60 websites with the highest in-link count\n",
        "\n",
        "nodes_dict = {}\n",
        "for item in new_node:\n",
        "  nodes_dict[item[0]] = item[1]\n",
        "\n",
        "inlinks = {}\n",
        "for node in new_node:\n",
        "  if node[0] not in inlinks:\n",
        "    inlinks[node[0]] = []\n",
        "\n",
        "for edge in edges_p:\n",
        "  inlinks[edge[1]].append(edge[0])\n",
        "\n",
        "inlinks_count = {}\n",
        "for item in inlinks.items():\n",
        "  inlinks_count[item[0]] = len(item[1])\n",
        "\n",
        "sorted_inlinks =  sorted(inlinks_count.items(), key = lambda kv: kv[1],reverse=True)\n",
        "\n",
        "xx = 0\n",
        "for item in sorted_inlinks:\n",
        "  print(\"id: \", item[0], \" Domain Name: \", nodes_dict[item[0]], \" Inlink count: \", item[1])\n",
        "  xx += 1\n",
        "  if xx == 20:\n",
        "    break\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id:  185524  Domain Name:  edu.mit.web  Inlink count:  4388\n",
            "id:  278032  Domain Name:  edu.stanford  Inlink count:  4021\n",
            "id:  244433  Domain Name:  edu.purdue.english.owl  Inlink count:  3531\n",
            "id:  140443  Domain Name:  edu.indiana  Inlink count:  3339\n",
            "id:  237176  Domain Name:  edu.princeton  Inlink count:  3251\n",
            "id:  64587  Domain Name:  edu.columbia  Inlink count:  3123\n",
            "id:  465503  Domain Name:  edu.yale  Inlink count:  2804\n",
            "id:  418623  Domain Name:  edu.utexas  Inlink count:  2622\n",
            "id:  383763  Domain Name:  edu.unc  Inlink count:  2592\n",
            "id:  197698  Domain Name:  edu.nap  Inlink count:  2494\n",
            "id:  439637  Domain Name:  edu.washington  Inlink count:  2291\n",
            "id:  373442  Domain Name:  edu.umich  Inlink count:  2281\n",
            "id:  440674  Domain Name:  edu.washington.depts  Inlink count:  2276\n",
            "id:  148945  Domain Name:  edu.jhu.muse  Inlink count:  2255\n",
            "id:  60975  Domain Name:  edu.colorado  Inlink count:  2232\n",
            "id:  449738  Domain Name:  edu.wisc  Inlink count:  2230\n",
            "id:  38320  Domain Name:  edu.bu  Inlink count:  2205\n",
            "id:  83572  Domain Name:  edu.dartmouth  Inlink count:  1965\n",
            "id:  408380  Domain Name:  edu.usc  Inlink count:  1952\n",
            "id:  178879  Domain Name:  edu.mit  Inlink count:  1946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uSlQEtmPTTA"
      },
      "source": [
        "Then, use the PageRank values compute by your second implementation. Note that some websites will have both a high in-link count and PageRank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwcci2kdPlMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7feb37-339e-482f-f6c5-5f6fc2b5c77d"
      },
      "source": [
        "# TODO: List the document ID, domain name, and PageRank of the 60 websites with the highest PageRank.\n",
        "\n",
        "PR_dict = {}\n",
        "for i, rank in enumerate(PR):\n",
        "  PR_dict[i] = rank\n",
        "\n",
        "sorted_PR =  sorted(PR_dict.items(), key = lambda kv: kv[1],reverse=True)\n",
        "\n",
        "xx = 0\n",
        "for item in sorted_PR:\n",
        "  print(\"id: \", item[0], \" Domain Name: \", nodes_dict[item[0]], \" Inlink count: \", item[1])\n",
        "  xx += 1\n",
        "  if xx == 20:\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id:  185524  Domain Name:  edu.mit.web  Inlink count:  0.0009287435339940903\n",
            "id:  465503  Domain Name:  edu.yale  Inlink count:  0.0006992387033356765\n",
            "id:  318278  Domain Name:  edu.ucdavis.cas  Inlink count:  0.0005984715790444966\n",
            "id:  278032  Domain Name:  edu.stanford  Inlink count:  0.0005949124171482952\n",
            "id:  136464  Domain Name:  edu.illinois  Inlink count:  0.0005799358302359056\n",
            "id:  237176  Domain Name:  edu.princeton  Inlink count:  0.0005022964971967422\n",
            "id:  319795  Domain Name:  edu.ucdavis.ice.code  Inlink count:  0.0004943562734855353\n",
            "id:  284517  Domain Name:  edu.stanford.web  Inlink count:  0.0004751176157734379\n",
            "id:  383763  Domain Name:  edu.unc  Inlink count:  0.0004721288382615311\n",
            "id:  64587  Domain Name:  edu.columbia  Inlink count:  0.00044770361757707794\n",
            "id:  449738  Domain Name:  edu.wisc  Inlink count:  0.0004363423554472278\n",
            "id:  60975  Domain Name:  edu.colorado  Inlink count:  0.0004326714113083783\n",
            "id:  346969  Domain Name:  edu.ucsf  Inlink count:  0.00043001199271247196\n",
            "id:  140443  Domain Name:  edu.indiana  Inlink count:  0.00042984371522444066\n",
            "id:  42502  Domain Name:  edu.byu.cas  Inlink count:  0.00041418052078680864\n",
            "id:  334739  Domain Name:  edu.uconn  Inlink count:  0.0004128198173473277\n",
            "id:  14945  Domain Name:  edu.arizona  Inlink count:  0.00040931672458022786\n",
            "id:  244433  Domain Name:  edu.purdue.english.owl  Inlink count:  0.0004033185112669418\n",
            "id:  373442  Domain Name:  edu.umich  Inlink count:  0.0004021760516369004\n",
            "id:  439637  Domain Name:  edu.washington  Inlink count:  0.00039873455789175263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxODBxL_Pyy2"
      },
      "source": [
        "Finally, compute some summary statistics on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD4bq6AyQIsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e39b48f-6f7d-419b-debb-44bb6ddf50a3"
      },
      "source": [
        "# TODO: Compute:\n",
        "# - the proportion of websites with no in-links (i.e., source nodes);\n",
        "\n",
        "no_inlink_counts = 0\n",
        "for item in inlinks.items():\n",
        "  if len(item[1]) == 0:\n",
        "    no_inlink_counts += 1\n",
        "\n",
        "\n",
        "print(\"proportion of websites with no in-links: \", no_inlink_counts / len(new_node))\n",
        "\n",
        "\n",
        "# - the proportion of websites with no out-links (i.e., sink nodes);\n",
        "\n",
        "no_outlink_counts = 0\n",
        "for item in outlinks.items():\n",
        "  if len(item[1]) == 0:\n",
        "    no_outlink_counts += 1\n",
        "\n",
        "\n",
        "print(\"proportion of websites with no in-links: \", no_outlink_counts / len(new_node))\n",
        "\n",
        "\n",
        "# - the proportion of websites whose PageRank is higher than the initial uniform distribution.\n",
        "\n",
        "initial_distribution_prob = 1 / len(new_node) \n",
        "print(initial_distribution_prob)\n",
        "\n",
        "count = 0\n",
        "for rank in PR:\n",
        "  if rank > initial_distribution_prob:\n",
        "    count += 1\n",
        "\n",
        "print(\"The proportion of websites whose PageRank is higher than the initial uniform distribution: \", count / len(PR) )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proportion of websites with no in-links:  0.26153633041013563\n",
            "proportion of websites with no in-links:  0.6106641661427643\n",
            "2.1311283258921438e-06\n",
            "The proportion of websites whose PageRank is higher than the initial uniform distribution:  0.15525696079789444\n"
          ]
        }
      ]
    }
  ]
}